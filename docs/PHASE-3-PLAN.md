# Phase 3: Smart Routing â€” Research & Implementation Plan

## Current Architecture (from codebase analysis)

### Request Flow
1. `src/index.ts` â†’ Hono app entry
2. Middleware chain: `requestLogger` â†’ `semanticCacheMiddleware` â†’ `errorHandler`
3. Routes: `/v1/chat/completions` (chat.ts), `/health` + `/metrics` (health.ts)
4. Provider selection: `routeModel(model)` in `src/services/router/index.ts`
5. Providers: OpenAI, Anthropic, Google via Vercel AI SDK adapters

### Current Provider Selection
- `src/config/routes.ts` â€” defines `RouteConfig` with primary + fallback targets
- `src/services/router/index.ts` â€” `routeModel()` resolves model string â†’ provider
- `src/services/providers/index.ts` â€” `detectProvider()` by model prefix, `getModel()` creates SDK instance
- Each provider (anthropic.ts, openai.ts, google.ts) has singleton pattern

### Integration Points for Phase 3
- **Routing rules** â†’ Replace/extend `routeModel()` in router service
- **Rate limiting** â†’ New middleware or pre-selection filter
- **Fallback** â†’ Extend existing `fallbacks` in RouteConfig
- **Latency tracking** â†’ Wrap provider calls, store in Redis or in-memory
- **Timeout** â†’ New Hono middleware before route handlers

---

## Research Findings

### Routing Strategies (from LiteLLM, Portkey analysis)

**LiteLLM approach:**
- Router with model list + routing strategies
- Strategies: `simple-shuffle`, `least-busy`, `latency-based-routing`, `cost-based-routing`
- Each deployment has RPM/TPM limits
- Fallbacks defined per model with retry logic
- Architecture: Fallbacks â†’ Retries â†’ Timeouts â†’ Cooldowns

**Recommended for us:**
- Start with cost-based + latency-based routing
- Use capability matching for model selection
- Keep it simpler than LiteLLM (we're not a proxy platform)

### Rate Limiting
- **Redis-based token bucket** (we already have Redis) â€” best fit
- Libraries: `node-rate-limiter` (in-memory) or custom Redis implementation
- LiteLLM uses Redis for distributed rate limiting with TPM/RPM counters

### Fallback/Retry
- **Circuit breaker** (opossum library) for provider health tracking
- Exponential backoff: 100ms â†’ 200ms â†’ 400ms (max 3 retries)
- On 5xx/429: retry same provider, then fallback to next
- Streaming retries: can't retry mid-stream, must restart from beginning

### Latency Tracking
- **EMA (Exponential Moving Average)** â€” recommended for irregular intervals
- Formula: `new_avg = alpha * sample + (1-alpha) * old_avg`
- Alpha = 0.3 gives good responsiveness
- Store per-provider, per-model in Redis or in-memory

### Cost Modeling
- Static pricing config (JSON file like LiteLLM's `model_prices_and_context_window.json`)
- Calculate: `(input_tokens * input_price + output_tokens * output_price) / 1000`
- Update pricing periodically

---

## Implementation Plan: 7 Parallel Workstreams

### Merge Order (sequential dependencies)
1. **Foundation** â†’ Types & interfaces (must merge first)
2-6. **Parallel** â†’ Rate Limiter, Latency Tracker, Timeout, Routing Rules, Fallback
7. **Model Selector** â†’ Orchestrator (depends on all above)

### Workstream 1: Foundation Types (S)
**Branch:** `feat/phase-3-foundation`
**New files:**
- `src/types/routing.ts` â€” RoutingRule, RuleCondition, RankedProvider
- `src/types/metrics.ts` â€” LatencyMetric
- `src/types/provider.ts` â€” ProviderState
- `src/config/routing-config.ts` â€” Zod schema for routing config

### Workstream 2: Rate Limiter (M)
**Branch:** `feat/phase-3-rate-limiter`
**New files:**
- `src/providers/rate-limiter.ts` â€” Token bucket per provider
- `src/providers/rate-limiter.test.ts`
**Depends on:** Workstream 1

### Workstream 3: Latency Tracker (M)
**Branch:** `feat/phase-3-latency-tracker`
**New files:**
- `src/metrics/latency-tracker.ts` â€” EMA + rolling window
- `src/metrics/aggregator.ts` â€” p50/p95/p99
- `src/metrics/latency-tracker.test.ts`
**Depends on:** Workstream 1

### Workstream 4: Timeout Middleware (S)
**Branch:** `feat/phase-3-timeout-handler`
**New files:**
- `src/middleware/timeout.ts` â€” AbortController-based
- `src/middleware/timeout.test.ts`
**Depends on:** Nothing

### Workstream 5: Routing Rules Engine (L)
**Branch:** `feat/phase-3-routing-rules`
**New files:**
- `src/routing/rules-engine.ts` â€” Rule evaluation + scoring
- `src/routing/rule-evaluator.ts` â€” Individual rule logic
- `src/routing/rules-engine.test.ts`
**Depends on:** Workstream 1

### Workstream 6: Fallback Handler (L)
**Branch:** `feat/phase-3-fallback-handler`
**New files:**
- `src/middleware/fallback.ts` â€” Retry + provider fallback
- `src/routing/retry-strategy.ts` â€” Backoff logic
- `src/middleware/fallback.test.ts`
**Depends on:** Workstream 1

### Workstream 7: Model Selector (M) â€” LAST
**Branch:** `feat/phase-3-model-selector`
**New files:**
- `src/routing/model-selector.ts` â€” Orchestrates all components
- `src/routing/model-selector.test.ts`
**Depends on:** ALL above

---

## Conflict Risk: ðŸŸ¢ LOW
- Workstreams 2-6 touch completely different files
- Only Model Selector imports from others
- Foundation merged first prevents type conflicts

## Execution with Git Worktrees
```bash
# After foundation is merged:
git worktree add ../phase3-rate-limiter feat/phase-3-rate-limiter
git worktree add ../phase3-latency feat/phase-3-latency-tracker
git worktree add ../phase3-timeout feat/phase-3-timeout-handler
git worktree add ../phase3-rules feat/phase-3-routing-rules
git worktree add ../phase3-fallback feat/phase-3-fallback-handler
```
